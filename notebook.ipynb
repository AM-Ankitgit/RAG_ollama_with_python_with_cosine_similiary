{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =\"Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer’s contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM)',\n",
       " ' transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result',\n",
       " ' transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements',\n",
       " ' not only in Natural Language Processing (NLP) tasks but also in a wide range of domains',\n",
       " ' including computer vision',\n",
       " ' audio and speech processing',\n",
       " ' healthcare',\n",
       " ' and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer’s contributions in specific fields',\n",
       " ' architectural differences',\n",
       " ' or performance evaluations',\n",
       " ' there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore',\n",
       " ' we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models',\n",
       " ' namely: NLP',\n",
       " ' Computer Vision',\n",
       " ' Multi-Modality',\n",
       " ' Audio and Speech Processing',\n",
       " ' and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers',\n",
       " ' thus contributing to the broader understanding of this groundbreaking technology.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_doc = corpus.split(\",\")\n",
    "corpus_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'wide range of  Natural Language Processing (NLP) '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wide', 'range', 'of', 'natural', 'language', 'processing', '(nlp)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embeddings\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "user_query_tokenization = user_query.lower().split()\n",
    "user_query_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'wide': 1,\n",
       "         'range': 1,\n",
       "         'of': 1,\n",
       "         'natural': 1,\n",
       "         'language': 1,\n",
       "         'processing': 1,\n",
       "         '(nlp)': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query_embedding  = Counter(user_query_tokenization)\n",
    "user_query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(nlp)',\n",
       " 'tasks',\n",
       " 'but',\n",
       " 'also',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'domains']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_doc_tokenization = \"Natural Language Processing (NLP) tasks but also in a wide range of domains\".lower().split()\n",
    "corpus_doc_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_doc_embedding  = Counter(corpus_doc_tokenization)\n",
    "corpus_doc_embedding\n",
    "\n",
    "\n",
    "lst = []\n",
    "#sentance vector\n",
    "for i in corpus_doc_embedding.values():\n",
    "    lst.append(i)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "language\n",
      "natural\n",
      "wide\n",
      "processing\n",
      "(nlp)\n",
      "range\n"
     ]
    }
   ],
   "source": [
    "# file the similarity between the query and the corups\n",
    "for token in corpus_doc_embedding.keys() & user_query_embedding.keys():\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform the cosine similarity search\n",
    "\n",
    "match_list =[]\n",
    "\n",
    "# find the vector \n",
    "for token in corpus_doc_embedding.keys() & user_query_embedding.keys():\n",
    "    match_list.append(corpus_doc_embedding[token]*user_query_embedding[token])\n",
    "\n",
    "match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product = sum(match_list)\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6457513110645907"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "query_magnitude = math.sqrt(sum(user_query_embedding[token] ** 2 for token in user_query_embedding))\n",
    "query_magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.605551275463989"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_magnitude = math.sqrt(sum(corpus_doc_embedding[token] ** 2 for token in corpus_doc_embedding))\n",
    "document_magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337993857053429"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score =(dot_product)/(query_magnitude*document_magnitude)\n",
    "similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarity:\n",
    "    def __init__(self,user_query,corpus) -> None:\n",
    "        self.user_query = user_query\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def get_cosine_similarity(self):\n",
    "        user_query_tokenization   = self.user_query.lower().split()\n",
    "        corpus_doc_tokenization  = self.corpus.lower().split()\n",
    "\n",
    "        user_query_embedding  = Counter(user_query_tokenization)\n",
    "        corpus_doc_embedding  = Counter(corpus_doc_tokenization)\n",
    "\n",
    "        dot_product  = sum([corpus_doc_embedding[token]*user_query_embedding[token] for token in corpus_doc_embedding.keys() & user_query_embedding.keys()])\n",
    "        query_magnitude = math.sqrt(sum(user_query_embedding[token] ** 2 for token in user_query_embedding))\n",
    "        document_magnitude = math.sqrt(sum(corpus_doc_embedding[token] ** 2 for token in corpus_doc_embedding))\n",
    "\n",
    "\n",
    "        # apply condition if query and corpus do not match the dot-product will be zero \n",
    "        similarity_score = dot_product / (query_magnitude * document_magnitude) if query_magnitude * document_magnitude != 0 else 0\n",
    "\n",
    "        return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337993857053429"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus_doc_= \"Natural Language Processing (NLP) tasks but also in a wide range of domains\"\n",
    "\n",
    "similarity_object = CosineSimilarity(user_query,corpus_doc_)\n",
    "similarity_object.get_cosine_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM)',\n",
       " ' transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result',\n",
       " ' transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements',\n",
       " ' not only in Natural Language Processing (NLP) tasks but also in a wide range of domains',\n",
       " ' including computer vision',\n",
       " ' audio and speech processing',\n",
       " ' healthcare',\n",
       " ' and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer’s contributions in specific fields',\n",
       " ' architectural differences',\n",
       " ' or performance evaluations',\n",
       " ' there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore',\n",
       " ' we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models',\n",
       " ' namely: NLP',\n",
       " ' Computer Vision',\n",
       " ' Multi-Modality',\n",
       " ' Audio and Speech Processing',\n",
       " ' and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers',\n",
       " ' thus contributing to the broader understanding of this groundbreaking technology.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_response(query, corpus_doc):\n",
    "    similarities = []\n",
    "    for doc in corpus_doc:\n",
    "\n",
    "        similarity_object = CosineSimilarity(query,doc)\n",
    "        similarity = similarity_object.get_cosine_similarity()\n",
    "        similarities.append(similarity)\n",
    "    return corpus_doc[similarities.index(max(similarities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer’s contributions in specific fields'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_response(\"published highlighting the transformer’s contributions\", corpus_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "full_response = []\n",
    "\n",
    "\n",
    "user_query = \"published highlighting the transformer’s contributions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models have been excelling in handling long dependencies between input sequence elements, and enable parallel processing.\n"
     ]
    }
   ],
   "source": [
    "full_response = []\n",
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "output should be in one line\n",
    "This is the recommended activity: {relevant_document}\n",
    "The user input is: {user_input}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"model\": \"llama2\",\n",
    "    \"prompt\": prompt.format(user_input=user_query, relevant_document=corpus_doc)\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "    \n",
    "    \n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
